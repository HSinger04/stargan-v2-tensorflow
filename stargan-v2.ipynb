{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89a790b5599e47929757c9be2d2b28bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57886bdac63c43878b048e0bdeca945e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5785946b054040c88b248ccb799fa2de",
              "IPY_MODEL_e915ab1b73964ad2ae3d71b158d9370f"
            ]
          }
        },
        "57886bdac63c43878b048e0bdeca945e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5785946b054040c88b248ccb799fa2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3441e964406c4d11ad9e6cc75be00972",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1246263,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1246263,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c595708858c94cbba4f7a9ab799dc16f"
          }
        },
        "e915ab1b73964ad2ae3d71b158d9370f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_389270c7b6f3452f853c87d5be5afea8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1246263/1246263 [00:17&lt;00:00, 72726.32it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61845f62ee5e4b989f527107f5faddbb"
          }
        },
        "3441e964406c4d11ad9e6cc75be00972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c595708858c94cbba4f7a9ab799dc16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "389270c7b6f3452f853c87d5be5afea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61845f62ee5e4b989f527107f5faddbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c92afa33c05467a9cff3b7f07da6a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1eb697aa99a64d2cb1b9e2eedee93c67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2759d8e7e53f4b0dbd738dd0b90c42b5",
              "IPY_MODEL_66ade53e0f14474282ce39daa4a213f3"
            ]
          }
        },
        "1eb697aa99a64d2cb1b9e2eedee93c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2759d8e7e53f4b0dbd738dd0b90c42b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_91d054f49c104f7ca486d556c067fb22",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1228546,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1228546,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31f7896c1c68402a842aa23abefa98bf"
          }
        },
        "66ade53e0f14474282ce39daa4a213f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ada1be710cac410985ac9affc0bd948b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1228546/1228546 [00:58&lt;00:00, 20876.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f52df7d3bb9407fadbb322dc1777ac1"
          }
        },
        "91d054f49c104f7ca486d556c067fb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31f7896c1c68402a842aa23abefa98bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ada1be710cac410985ac9affc0bd948b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f52df7d3bb9407fadbb322dc1777ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQntiASAkVk5"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HSinger04/stargan-v2-tensorflow/blob/master/stargan-v2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I_WVtoi81lC"
      },
      "source": [
        "<h1> Practical 3 - Language Modeling <h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L-ucuhN9HfE"
      },
      "source": [
        "After we have concerned ourselves with word embeddings and text classification in the previous notebooks, we will now focus on language modeling. </br>\n",
        "\n",
        "In the last practical we worked on text classification as a problem that could be solved with deep learning. Today, we will get to know a different application of RNN's, namely language modeling. </br>\n",
        "\n",
        "Word prediction is a Natural Language Processing - NLP application concerned with predicting the next word given the preceding text. Auto-complete or suggested responses are popular types of language prediction. The first step towards language prediction is the selection of a language model. </br>\n",
        "\n",
        "There are generally two models you can use to develop Next Word Predictor:\n",
        "</br> 1) statistical N-gram model or \n",
        "</br> 2) Neural Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyhaqQQD9OM-"
      },
      "source": [
        "**0. Task (0 points)**  </br>\n",
        "As usual, before we dive into the tasks here are a couple imports we will later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ztGGH--rfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c362a0-56bf-4180-86af-63d337f1c35d"
      },
      "source": [
        "!pip install boltons -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10kB 31.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40kB 19.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 18.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ_sI6SI-UIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ee6b79-1686-4efc-fc21-88adc67a8bcb"
      },
      "source": [
        "import string\n",
        "from pathlib import Path\n",
        "from textwrap import wrap\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from boltons.iterutils import windowed\n",
        "#from tqdm import tqdm_notebook\n",
        "#from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXUE8n_v-UIs",
        "outputId": "c26b6104-e240-42a9-ddd8-2517e965bcea"
      },
      "source": [
        "device_word = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device_char = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZy9leGX-cNk"
      },
      "source": [
        "<h1>Word and Text Generation</h1>\r\n",
        "\r\n",
        "In this notebook we will do two things:\r\n",
        "1.   Generate a RNN, that can Learn how english characters are combined.\r\n",
        "2.   Generate a RNN, that can Learn how english words are combined.\r\n",
        "\r\n",
        "Therefore we are going to do the following steps:\r\n",
        "1.   Load the Data\r\n",
        "2.   Preprocess the Data for character-level generation.\r\n",
        "3.   Preprocess the Data for word-level generation.\r\n",
        "4.   Building an RNN\r\n",
        "5.   Applying the RNN on the Data of Step 1.\r\n",
        "6.   Applying the RNN on the Data of Step 2.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atLUvUh-MbuZ"
      },
      "source": [
        "<h2>1. Load the Data</h2>\r\n",
        "\r\n",
        "Our Dataset consists of multiple texts regarding weight loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKw4lpSO-UI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ce7e76-e0fb-4a21-f4f0-0040be9d0533"
      },
      "source": [
        "#We can find the articles here:\n",
        "DATA_PATH = 'data/weight_loss/articles.jsonl'\n",
        "if not Path(DATA_PATH).is_file():\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id='1mafPreWzE-FyLI0K-MUsXPcnUI0epIcI',\n",
        "        dest_path='data/weight_loss/weight_loss_articles.zip',\n",
        "        unzip=True,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1mafPreWzE-FyLI0K-MUsXPcnUI0epIcI into data/weight_loss/weight_loss_articles.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KtTqxTN-byG",
        "outputId": "b7fdab66-e7e6-41aa-a4ef-9ee3856983b7"
      },
      "source": [
        "#lets print out the first article\r\n",
        "print(pd.read_json(DATA_PATH).text.str.lower().tolist()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight gaining is a common problem around the world. in developed country, it is the most common problem. in this article, i am not going to show you some advance and magical technique which will make you slim overnight. i am going to show you tips on the basis of real facts which works. in this article, i will give you how to tips, which will help you to lose weight. are you ready?\n",
            "calories requirement\n",
            "first thing you need to understand is why you gain weight. why? whenever you eat or drink something, you will get some calories. when you think about weight, everything revolves around calories.\n",
            "whatever you do, will burn some calories no matter how small work it is or just a movement of your body. your body burns thousands of calories in one day.\n",
            "if you are getting more calories than needed, you will gain weight. if you are getting fewer calories than needed, you will lose weight. so for losing weight, you need to know how much calorie your body required.\n",
            "find require calories for your body\n",
            "how will you know that how much calories your body require? well, there is a formula to find out that but you don't need to understand that. just go tohttp://www.freedieting.com/tools/calorie_calculator.htm. enter the details and you will come to know how much calories require for your body.\n",
            "now you know how much calories your body needs. you just need to balance between require and burning calories. there are two ways you can lose weight.\n",
            "you are not going to use only one. you will use mix of both. you will consume less calories and burn more calories. so keep a track of how much calories you take. also do some more exercise. not too much just a little more. so you burn more calories.\n",
            "you are controlling calories in two ways. losing little more calories and getting little less calories. so you will definitely lose weight.\n",
            "take less calories\n",
            "burn more calories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXP6jn83Orbg"
      },
      "source": [
        "<h2>2. Preprocessing the data for sequence generation</h2>\r\n",
        "\r\n",
        "As you can see in the cell above it is pretty tedious to access the data. In the next few steps we help you and the network to access the data easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0lWXlh6V-GC"
      },
      "source": [
        "def remove_unprintable_chars(all_chars_windowed):\r\n",
        "  not_printbl_chars=[]\r\n",
        "  filtered_chars=[]\r\n",
        "  printbl=True\r\n",
        "  for sequence in tqdm(all_chars_windowed):\r\n",
        "    printbl=True\r\n",
        "    for char in sequence:\r\n",
        "      if not(char in string.printable):  \r\n",
        "        printbl=False\r\n",
        "        not_printbl_chars+=[char]\r\n",
        "    if printbl==True:\r\n",
        "      filtered_chars+=[sequence]\r\n",
        "  return filtered_chars \r\n",
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe1X3_lY-UI9"
      },
      "source": [
        "def textlist_generator(path):\n",
        "  return pd.read_json(path).text.str.lower().tolist()\n",
        "\n",
        "def load_data_char(path, sequence_length=125):\n",
        "\n",
        "    # Generate a list of texts from the dataset\n",
        "    texts = textlist_generator(path)\n",
        "    #print(texts[0] + \"\\n\")\n",
        "\n",
        "\n",
        "    chars_windowed = [list(windowed(text, sequence_length)) for text in texts]\n",
        "    #print(chars_windowed[:2])\n",
        "    \n",
        "    all_chars_windowed = [sublst for lst in chars_windowed for sublst in lst]\n",
        "    #print(all_chars_windowed[:2])\n",
        "\n",
        "    filtered_chars = remove_unprintable_chars(all_chars_windowed)\n",
        "    #print(filtered_chars[:2])\n",
        "    return filtered_chars\n",
        "\n",
        "\n",
        "def set_of_chars_in(sequences):\n",
        "    return {sublst for lst in sequences for sublst in lst}\n",
        "\n",
        "\n",
        "def create_char2idx(sequences):\n",
        "    set_of_chars = set_of_chars_in(sequences)\n",
        "    return {char: idx for idx, char in enumerate(sorted(set_of_chars))}\n",
        "\n",
        "\n",
        "def encode_sequence(sequence, char2idx):\n",
        "    return [char2idx[char] for char in sequence]\n",
        "\n",
        "\n",
        "def encode_sequences(sequences, char2idx):\n",
        "    return np.array([\n",
        "        encode_sequence(sequence, char2idx) \n",
        "        for sequence in tqdm(sequences)\n",
        "    ])\n",
        "\n",
        "\n",
        "class Sequences(Dataset):\n",
        "    def __init__(self, path, sequence_length=125):\n",
        "        self.sequences = load_data_char(DATA_PATH, sequence_length=sequence_length)\n",
        "        self.vocab_size = len(set_of_chars_in(self.sequences))\n",
        "        self.char2idx = create_char2idx(self.sequences)\n",
        "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
        "        self.encoded = encode_sequences(self.sequences, self.char2idx)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        return self.encoded[i, :-1], self.encoded[i, 1:]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iIT4s4qfrY"
      },
      "source": [
        "The Tasks 2.1 to 2.3 will help you to understand the code better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIYI0mHadM6p"
      },
      "source": [
        "**2.1** <br>\r\n",
        "(1 Point)\r\n",
        "Describe the variable `chars_windowed`. What does it contain? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xViMgbwEeU_u"
      },
      "source": [
        "For each `text` in `texts`, `chars_windowed` contains a list of character windows as tuples. Each of those tuples have length `sequence_length`. The first tuple contains the characters from `text[0:sequence_length]`, the second the characters from\n",
        "`text[1:sequence_length+1]`, or in general for `0 <= i <= len(text)-sequence_length`, the i-th tuple contains the characters from `text[i:sequence_length+i]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wev3uOKRdq5I"
      },
      "source": [
        "**2.2** <br>\r\n",
        "(1 Point)\r\n",
        "Describe the variable `all_chars_windowed`. What does it contain?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYQulPqkeTmv"
      },
      "source": [
        "`all_chars_windowed` is the up to tupel level flattened version of `chars_windowed` i.e. the list that contains all the tuples from `chars_windowed` in the form of a list of tupels, while with `chars_windowed`, we had a list of lists of tuples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB7cFHZ4Z_Gh"
      },
      "source": [
        "**2.3** <br>\r\n",
        "(1 Point)\r\n",
        "Explain shortly, what the function `remove_unprintable_chars(all_chars_windowed)`does. We do not want a step by step explaination, just describe the generall idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO5EoJXFanlq"
      },
      "source": [
        "`remove_unprintable_chars(all_chars_windowed)` filters out all window tuples that contain a character which is not printable i.e. are not an element of `string.printable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZHxjVRgbQgs"
      },
      "source": [
        "Now lets load our char_dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQh-02Ye89Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d46f4e2-8d25-4200-8220-c56830312acf"
      },
      "source": [
        "sequence_length=int(input(\"choose your sequence_length (for this task, we did choose 128): \"))\r\n",
        "if sequence_length<=1:\r\n",
        "  print(\"1 or less is not a valid sequence length. Your model will not learn anything from just one word at a time. The sequence lenght of 128 has been chosen for you.\")\r\n",
        "  sequence_length=128"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "choose your sequence_length (for this task, we did choose 128): 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "89a790b5599e47929757c9be2d2b28bc",
            "57886bdac63c43878b048e0bdeca945e",
            "5785946b054040c88b248ccb799fa2de",
            "e915ab1b73964ad2ae3d71b158d9370f",
            "3441e964406c4d11ad9e6cc75be00972",
            "c595708858c94cbba4f7a9ab799dc16f",
            "389270c7b6f3452f853c87d5be5afea8",
            "61845f62ee5e4b989f527107f5faddbb",
            "9c92afa33c05467a9cff3b7f07da6a68",
            "1eb697aa99a64d2cb1b9e2eedee93c67",
            "2759d8e7e53f4b0dbd738dd0b90c42b5",
            "66ade53e0f14474282ce39daa4a213f3",
            "91d054f49c104f7ca486d556c067fb22",
            "31f7896c1c68402a842aa23abefa98bf",
            "ada1be710cac410985ac9affc0bd948b",
            "2f52df7d3bb9407fadbb322dc1777ac1"
          ]
        },
        "id": "BZxQzaME-UJG",
        "outputId": "b3294aeb-10bd-4fc8-8929-a5a0ec8ab482"
      },
      "source": [
        "dataset_char = Sequences(DATA_PATH, sequence_length=128)\n",
        "len(dataset_char)\n",
        "train_loader_char = DataLoader(dataset_char, batch_size=4096)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89a790b5599e47929757c9be2d2b28bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1246263.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c92afa33c05467a9cff3b7f07da6a68",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1228546.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9feqYMs_vYS"
      },
      "source": [
        "<h2>3. Preprocessing the data for word-level generation</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcDtOiP7A4oj"
      },
      "source": [
        "We now have to do the same preprocessing steps for our word-level-model. But dont worry, it works quite similar to the character-level-preprocessing steps. In the following, there are a couple of tasks, that will guide you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mqFYOX7Ukw9"
      },
      "source": [
        "**3.1 Tokenize**<br>\r\n",
        "(3 Point)\r\n",
        "Complete the function `tokenize` which gets multiple texts and returns a list, which consists of a list of word-level-tokens for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk6WT1WYU-he"
      },
      "source": [
        "def tokenize(texts):   \r\n",
        "    texts_tokens = [word_tokenize(text) for text in texts]\r\n",
        "    return texts_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OonqPi-lWlh8"
      },
      "source": [
        "**3.2 Unprintable Sequences**<br>\r\n",
        "(4 Points)\r\n",
        "Do you remember the task 2.3? Apply the same functionality, but keep in mind, that we are now working on the basis of words, not chars. \r\n",
        "Adapt the function from task 2.3 so that it now works with words. We still want to delete the sequences. Write your solution into the function `remove_unprintable_sequences2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drlYnhUfX6g6"
      },
      "source": [
        "def remove_unprintable_sequences2(all_words_windowed):\r\n",
        "  filtered_words = []\r\n",
        "  for word_tuple in all_words_windowed:\r\n",
        "    for word in word_tuple:\r\n",
        "      break_out = False\r\n",
        "      for char in word:\r\n",
        "        if not char in string.printable:\r\n",
        "          break_out = True\r\n",
        "          break\r\n",
        "      if break_out:\r\n",
        "        break    \r\n",
        "    else:\r\n",
        "      filtered_words+=[word_tuple]\r\n",
        "\r\n",
        "  return filtered_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y5YSvESboYU"
      },
      "source": [
        "We now put all the functions you provided in this new function, to load our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCVbIY0Tqfp"
      },
      "source": [
        "def load_data_word(path, sequence_length=5):\r\n",
        "\r\n",
        "    #Generate a list of texts from the dataset\r\n",
        "    texts = textlist_generator(path)\r\n",
        "\r\n",
        "    texts=tokenize(texts)\r\n",
        "\r\n",
        "    words_windowed = [list(windowed(text, sequence_length)) for text in texts]\r\n",
        "\r\n",
        "\r\n",
        "    all_words_windowed = [sublst for lst in words_windowed for sublst in lst]\r\n",
        "\r\n",
        "    filtered_words = remove_unprintable_sequences2(all_words_windowed)\r\n",
        "\r\n",
        "    return filtered_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZ3bKU1C4xm"
      },
      "source": [
        "**** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuUnW1w8EMo5"
      },
      "source": [
        "**3.3** </br>\n",
        "(1 Points)\n",
        "Write a function that returns a set of all the words in the sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0kyCSQOEWuW"
      },
      "source": [
        "def set_of_words_in(sequences):\n",
        "    set_of_words = set([word for word_tuple in sequences for word in word_tuple])\n",
        "    return set_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B102b9njEe7U"
      },
      "source": [
        "**3.4** </br>\n",
        "(1 Points)\n",
        "Write a function that returns a dictionary containing the set of words indentified in task 3.3 and assigns an index to each of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxj_ahttEzJl"
      },
      "source": [
        "def create_word2idx(sequences):\n",
        "    word2idx = {}\n",
        "    set_of_words = set_of_words_in(sequences)\n",
        "    for i, word in enumerate(list(set_of_words)):\n",
        "      word2idx[word] = i  \n",
        "    return word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqaN_y1GE5co"
      },
      "source": [
        "**3.5** </br>\n",
        "(1 Points)\n",
        "Create a function `encode_sequence`, that transforms the words of a list of words `sequence` into their equivalent index from `word2index`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa2RstHQFLux"
      },
      "source": [
        "def encode_sequence(sequence, word2idx):\n",
        "    encoded_seq = [word2idx[word] for word in sequence]\n",
        "    return encoded_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7dNIOZKGmt7"
      },
      "source": [
        "**3.6** </br>\n",
        "(1 Points)\n",
        "Complete the function `encode_sequences` that generates a numpy array, with the encoded sequence of all the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVK1-y5ZGsDH"
      },
      "source": [
        "def encode_sequences(sequences, word2idx):\n",
        "    return np.array([encode_sequence(sequence, word2idx) for sequence in sequences])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNi7RhbUFs1i"
      },
      "source": [
        "In the next code snippet we call all the functions you defined above. (You dont have to do anything here, just run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRJ1D6OtFrd7"
      },
      "source": [
        "class Sequences(Dataset):\n",
        "    def __init__(self, path, sequence_length=30):\n",
        "        self.sequences = load_data_word(DATA_PATH, sequence_length=sequence_length)\n",
        "        self.vocab_size = len(set_of_words_in(self.sequences))\n",
        "        self.word2idx = create_word2idx(self.sequences)\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "        self.encoded = encode_sequences(self.sequences, self.word2idx)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        return self.encoded[i, :-1], self.encoded[i, 1:]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4PRO9L7uOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea28dc9-da46-43a0-f69d-a0cbb2335bc5"
      },
      "source": [
        "sequence_length=int(input(\"choose your sequence_length (for this task, we did choose 10): \"))\r\n",
        "sequence_length = 10\r\n",
        "if sequence_length<=1:\r\n",
        "  print(\"1 or less is not a valid sequence length. Your model will not learn anything from just one word at a time. The sequence lenght of 10 has been chosen for you.\")\r\n",
        "  sequence_length=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "choose your sequence_length (for this task, we did choose 10): 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSV6H2z9IqV5"
      },
      "source": [
        "dataset_word = Sequences(DATA_PATH, sequence_length)\r\n",
        "len(dataset_word)\r\n",
        "train_loader_word = DataLoader(dataset_word, batch_size=4096)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyoQrbvtdLy"
      },
      "source": [
        "<h2>4. char-RNN: Character-level text generation</h2>\n",
        "\n",
        "For an Idea how it could be done read this [Blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). In there they are using an LSTM to generate new texts on character Level. To make it easier for you, in this notebook we are Building a RNN with GRU as basis. GRU works close to LSTM, but is easier to handle. For a full comparison of these two have a look at [this](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gAAAtscOJf-"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dimension=100,\n",
        "        hidden_size=128, \n",
        "        n_layers=1,\n",
        "        device='cpu',\n",
        "    ):\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        \n",
        "        #Our Building-Blocks:\n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n",
        "        self.rnn = nn.GRU(\n",
        "            embedding_dimension,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        #we initialize a random hidden state\n",
        "        return torch.randn(self.n_layers, batch_size, self.hidden_size).to(self.device)\n",
        "    \n",
        "    def forward(self, input_, hidden):\n",
        "        #we feed the input through our RNN\n",
        "        encoded = self.encoder(input_)\n",
        "        output, hidden = self.rnn(encoded.unsqueeze(1), hidden)\n",
        "        output = self.decoder(output.squeeze(1))\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys33f5T5jF8K"
      },
      "source": [
        "Lets Initialize the model with the data for chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrC85qKz-UJT"
      },
      "source": [
        "model_char = RNN(vocab_size=dataset_char.vocab_size, device=device_char).to(device_char)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_char.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw0OpXVz9S7n"
      },
      "source": [
        "print(model_char)\n",
        "print()\n",
        "print('Trainable parameters:')\n",
        "print('\\n'.join([' * ' + x[0] for x in model_char.named_parameters() if x[1].requires_grad]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xRYf-fjulU"
      },
      "source": [
        "![](https://github.com/HSinger04/DL4NLP/blob/master/src/images/char_rnn_diagram.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edy_iSXh-UJZ"
      },
      "source": [
        "model_char.train()\n",
        "train_losses = []\n",
        "for epoch in range(30):\n",
        "    progress_bar = tqdm(train_loader_char, leave=False)\n",
        "    losses = []\n",
        "    total = 0\n",
        "    for inputs, targets in progress_bar:\n",
        "        batch_size = inputs.size(0)\n",
        "        hidden = model_char.init_hidden(batch_size)\n",
        "\n",
        "        model_char.zero_grad()\n",
        "        \n",
        "        loss = 0\n",
        "        for char_idx in range(inputs.size(1)):\n",
        "            output, hidden = model_char(inputs[:, char_idx].to(device_char), hidden)\n",
        "            loss += criterion(output, targets[:, char_idx].to(device_char))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        avg_loss = loss.item() / inputs.size(1)\n",
        "        \n",
        "        progress_bar.set_description(f'Loss: {avg_loss:.3f}')\n",
        "        \n",
        "        losses.append(avg_loss)\n",
        "        total += 1\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    train_losses.append(epoch_loss)\n",
        "        \n",
        "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')\n",
        "\n",
        "\n",
        "#Again, this will take a while. Go for a walk in the sunshine or something else nice :D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5KJlq_yk4xi"
      },
      "source": [
        "Now lets test this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsxjlCe9-UJd"
      },
      "source": [
        "def pretty_print(text):\n",
        "    \"\"\"Wrap text for nice printing.\"\"\"\n",
        "    to_print = ''\n",
        "    for paragraph in text.split('\\n'):\n",
        "        to_print += '\\n'.join(wrap(paragraph))\n",
        "        to_print += '\\n'\n",
        "    print(to_print)\n",
        "\n",
        "#this temperature defines how \"strict\" the RNN is following the original\n",
        "temperature = 1.0\n",
        "\n",
        "model_char.eval()\n",
        "seed = '\\n'\n",
        "text = ''\n",
        "with torch.no_grad():\n",
        "    batch_size = 1\n",
        "    hidden = model_char.init_hidden(batch_size)\n",
        "    last_char = dataset_char.char2idx[seed]\n",
        "    for _ in range(1000):\n",
        "        output, hidden = model_char(torch.LongTensor([last_char]).to(device_char), hidden)\n",
        "        \n",
        "        #find the next char\n",
        "        distribution = output.squeeze().div(temperature).exp()\n",
        "        guess = torch.multinomial(distribution, 1).item()\n",
        "        \n",
        "        #the next char is the new last_char\n",
        "        last_char = guess\n",
        "\n",
        "        #append char to text.\n",
        "        text += dataset_char.idx2char[guess]\n",
        "\n",
        "  \n",
        "pretty_print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhUG73OHib1R"
      },
      "source": [
        "Even though it may not be sentences, those words already sound like they are out of the mouth of your fitness coach. Maybe our word-level-model can do more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqXvJapdIMCi"
      },
      "source": [
        "<h2>5. Word-RNN: Word-level text generation</h2>\r\n",
        "\r\n",
        "Since the Dataset is originally made for char-level-generation this of course is probably not appropriate for word-level-generator. Just as a prove of concept, we will still show you how it works. The results are still quite good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXx0-gArPPjd"
      },
      "source": [
        "model_word = RNN(vocab_size=dataset_word.vocab_size, device=device_word).to(device_word)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model_word.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx8L3RGXPPje"
      },
      "source": [
        "print(model_word)\n",
        "print()\n",
        "print('Trainable parameters:')\n",
        "print('\\n'.join([' * ' + x[0] for x in model_word.named_parameters() if x[1].requires_grad]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLDenti0PPjf"
      },
      "source": [
        "![](https://github.com/HSinger04/DL4NLP/blob/master/src/images/char_rnn_diagram.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SLh24QhgT3h"
      },
      "source": [
        "**5.1**\r\n",
        "<br>\r\n",
        "(4 Points)\r\n",
        "Now that you have seen, how our character-level model is trained, it is time to do the same for our word-level-model.\r\n",
        "Now it is your turn to implement the loss function. Have a look at the code from char-level-generation as an orientation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz1lP9LcgUYS"
      },
      "source": [
        "\r\n",
        "model_word.train()\r\n",
        "train_losses = []\r\n",
        "for epoch in range(50):\r\n",
        "    progress_bar = tqdm(train_loader_word, leave=False)\r\n",
        "    losses = []\r\n",
        "    total = 0\r\n",
        "    for inputs, targets in progress_bar:\r\n",
        "        batch_size = inputs.size(0)\r\n",
        "        hidden = model_word.init_hidden(batch_size)\r\n",
        "\r\n",
        "        model_word.zero_grad()\r\n",
        "        \r\n",
        "        loss = 0\r\n",
        "        for word_idx in range(inputs.size(1)):\r\n",
        "            output, hidden = model_word(inputs[:, word_idx].to(device_word), hidden)\r\n",
        "            loss += criterion(output, targets[:, word_idx].to(device_word))\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        avg_loss = loss.item() / inputs.size(1)\r\n",
        "        \r\n",
        "        progress_bar.set_description(f'Loss: {avg_loss:.3f}')\r\n",
        "        \r\n",
        "        losses.append(avg_loss)\r\n",
        "        total += 1\r\n",
        "    \r\n",
        "    epoch_loss = sum(losses) / total\r\n",
        "    train_losses.append(epoch_loss)\r\n",
        "        \r\n",
        "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np52XAlcg_nD"
      },
      "source": [
        "Big finale: Now we want you to test your model: Try it out and look if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYjYvLJEPPjg"
      },
      "source": [
        "def pretty_print(text):\n",
        "    \"\"\"Wrap text for nice printing.\"\"\"\n",
        "    to_print = ''\n",
        "    for paragraph in text.split('\\n'):\n",
        "        to_print += '\\n'.join(wrap(paragraph))\n",
        "        to_print += '\\n'\n",
        "    print(to_print)\n",
        "\n",
        "\n",
        "\n",
        "def generate(keywords, model_word):\n",
        "\n",
        "\n",
        "  keywords=keywords.lower()\n",
        "  text_tokens=[]\n",
        "  #for every sentence.,.\n",
        "  texts_sent=sent_tokenize(keywords)\n",
        "  for sent in texts_sent:\n",
        "    #we seperate the sentece into words...\n",
        "    sent=word_tokenize(sent)\n",
        "    #...and add these words into this list\n",
        "    for token in sent:\n",
        "      text_tokens+=[token+\" \"]\n",
        "  #our seed is only the last word of your input\n",
        "  seed=text_tokens[-1]\n",
        "  \n",
        "  #check if your word is even in the training-data\n",
        "  try:\n",
        "    seed = seed[:-1]\n",
        "    dataset_word.word2idx[seed]\n",
        "  except KeyError:\n",
        "    print(\"the Word\",seed,\"is not part of the learned words and therefore can not be used as starting point for the new text\")\n",
        "\n",
        "  temperature = 1.0 \n",
        "\n",
        "  model_word.eval()\n",
        "  text = \"\"\n",
        "  with torch.no_grad():\n",
        "      batch_size = 1\n",
        "      hidden = model_word.init_hidden(batch_size)\n",
        "      last_word = dataset_word.word2idx[seed]\n",
        "      for _ in range(100):\n",
        "          output, hidden = model_word(torch.LongTensor([last_word]).to(device_word), hidden)\n",
        "          \n",
        "          distribution = output.squeeze().div(temperature).exp()\n",
        "          guess = torch.multinomial(distribution, 1).item()\n",
        "          \n",
        "          last_word = guess\n",
        "          text += dataset_word.idx2word[guess]\n",
        "          text += \" \"\n",
        "  return text     \n",
        "\n",
        "\n",
        "keywords=input(\"Start your text about fitness with a few words: \")\n",
        "print(\"\\n\")\n",
        "\n",
        "text=generate(keywords, model_word)\n",
        "pretty_print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXArrWduviN"
      },
      "source": [
        "Now you can look at the resuls of your Char-Level and Word-Level-Model. This of course is not graded, but might be interesting for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJY0Jw108xR"
      },
      "source": [
        "Congratulation! You are done now"
      ]
    }
  ]
}